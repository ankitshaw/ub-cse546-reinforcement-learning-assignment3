{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /usr/local/python/3.10.4/lib/python3.10/site-packages (4.64.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch \n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gym\n",
    "\n",
    "from collections import deque\n",
    "from PIL import Image\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Actor, self).__init__()\n",
    "        self.linear1 = nn.Linear(in_features=input_size, out_features=128)\n",
    "        self.linear2 = nn.Linear(in_features=128, out_features=output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.relu(self.linear2(x))\n",
    "        output = F.softmax(x, dim=-1)\n",
    "        return output\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.linear1 = nn.Linear(in_features=input_size, out_features=128)\n",
    "        self.linear2 = nn.Linear(in_features=128, out_features=output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.linear1(x))\n",
    "        output = self.linear2(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class CartPoleVanillaDQN():\n",
    "    def __init__(self, env, typ=\"dense\"):\n",
    "      self.env = env\n",
    "      self.typ = typ\n",
    "      self.max_episodes = 10000\n",
    "      self.max_steps = 500\n",
    "      self.gamma = 0.999\n",
    "      self.epsilon = 1.0\n",
    "      self.epsilon_min = 0.001\n",
    "      self.epsilon_decay_lamda = self.epsilon_min**(1/self.max_episodes)\n",
    "      self.target_update_counter = 0\n",
    "      self.target_update_frequency = 10\n",
    "      self.buffer_minibatch_size = 120\n",
    "      self.replay_buffer_capacity=5000\n",
    "      self.replay_buffer = deque([],maxlen=self.replay_buffer_capacity)\n",
    "      self.resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize(84, interpolation=Image.CUBIC),\n",
    "                    T.Grayscale(),\n",
    "                    T.ToTensor()])  \n",
    "      env.reset()\n",
    "      _, _, self.state_screen_h, self.state_screen_w = self.screen_preprocessor(self.env.render()).shape\n",
    "      self.state_count = env.observation_space.shape[0]\n",
    "      self.action_count = env.action_space.n\n",
    "      self.actor = Actor(self.state_count, self.action_count) if self.typ==\"dense\" else None\n",
    "      self.critic = Critic(self.state_count, 1) if self.typ==\"dense\" else None\n",
    "      # self.rms_optimizer = optim.RMSprop(self.q_value_dqn.parameters())#, lr=0.001)#, weight_decay=0.05)\n",
    "      # self.loss_func = F.smooth_l1_loss\n",
    "      self.actor_optimizer = optim.SGD(self.actor.parameters(), lr=0.0001)#optim.RMSprop(self.q_value_dqn.parameters(), lr=0.001)#, weight_decay=0.05)\n",
    "      self.critic_optimizer = optim.SGD(self.critic.parameters(), lr=0.0001)\n",
    "      self.loss_func = F.mse_loss #F.smooth_l1_loss\n",
    "      \n",
    "      self.timestep_list = []\n",
    "      self.rewards_list = []\n",
    "      self.epsilon_list = []\n",
    "      \n",
    "      self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "      # self.q_value_dqn#.to(self.device)\n",
    "      # self.target_dqn#.to(self.device)\n",
    "\n",
    "      self.test_max_episodes = 10\n",
    "      self.test_timestep_list = []\n",
    "      self.test_rewards_list = []\n",
    "      self.test_epsilon_list = []\n",
    "      self.trained_policy_path = \"./baseline/cartpole.pth\"\n",
    "\n",
    "    def initialize_buffer(self):\n",
    "      for i in range(self.replay_buffer_capacity):\n",
    "        self.env.reset()\n",
    "        current_state = self.screen_preprocessor(self.env.render())\n",
    "        done = False\n",
    "        while not done:  \n",
    "          current_action = self.choose_action(current_state)#.to(self.device)\n",
    "          curr_obs, curr_reward, done, truncated, info = self.env.step(current_action.item())\n",
    "          next_state = self.screen_preprocessor(self.env.render())\n",
    "\n",
    "          self.add_to_replay_buffer((current_state, current_action, curr_reward, next_state, done))\n",
    "          current_state = next_state\n",
    "\n",
    "    def screen_preprocessor(self,state_screen):\n",
    "      def crop(variable,tw,th):\n",
    "         c, h, w = variable.shape\n",
    "         x1 = int(round((w - tw) / 2.))\n",
    "         y1 = int(round((h - th) / 2.))\n",
    "         return variable[:,y1:y1+th,x1:x1+tw]\n",
    "      state_screen = state_screen.transpose((2,0,1))\n",
    "      screen_c, screen_h, screen_w = state_screen.shape\n",
    "      screen = self.resize(torch.from_numpy(state_screen))\n",
    "      screen = crop(screen, 60, 60)\n",
    "      screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "      screen = torch.from_numpy(screen)\n",
    "      return screen.unsqueeze(0)\n",
    "    \n",
    "    def choose_action(self, state, test=False):\n",
    "      random_no = np.random.random()\n",
    "      if self.epsilon > random_no and not test:\n",
    "          return torch.tensor([[np.random.choice(self.action_count)]])\n",
    "      else:\n",
    "          return self.get_q_value_for_state(state)#.to(self.device))\n",
    "    \n",
    "    def get_q_value_for_state(self, state):\n",
    "      q_values = self.q_value_dqn(state)\n",
    "      return q_values.max(1)[1].view(1, 1)\n",
    "\n",
    "    def eval(self):\n",
    "      self.q_value_dqn.load_state_dict(torch.load(self.trained_policy_path))\n",
    "\n",
    "      for episode in tqdm(range(0, self.test_max_episodes), unit='episodes'):\n",
    "        obs = self.env.reset()[0]\n",
    "        done = False\n",
    "        reward_sum = 0\n",
    "        steps = 0\n",
    "        current_state = torch.from_numpy(obs).unsqueeze(0) if self.typ==\"dense\" else self.screen_preprocessor(self.env.render())\n",
    "        while not done:\n",
    "          tt = time.time()\n",
    "\n",
    "          current_action = self.choose_action(current_state, test=True)#.to(self.device)\n",
    "          curr_obs, curr_reward, done, truncated, info = self.env.step(current_action.item())\n",
    "          self.env.render()\n",
    "          \n",
    "          reward_sum += curr_reward\n",
    "          next_state = torch.from_numpy(curr_obs).unsqueeze(0) if self.typ==\"dense\" else self.screen_preprocessor(self.env.render())\n",
    "\n",
    "          steps +=1\n",
    "          \n",
    "          current_state = next_state\n",
    "          if steps==500 or reward_sum==470:\n",
    "            done=True\n",
    "\n",
    "        self.update_target_net(episode)\n",
    "            \n",
    "        self.test_timestep_list.append(steps)\n",
    "        self.test_rewards_list.append(reward_sum)\n",
    "\n",
    "\n",
    "    def train(self, current_state, reward, next_state, done, action_log_prob, I):\n",
    "      \n",
    "      current_state_val = self.critic(current_state)\n",
    "      next_state_val = self.critic(next_state) if not done else 0\n",
    "  \n",
    "      delta = reward + self.gamma*next_state_val - current_state_val\n",
    "      actor_loss = -1*I*action_log_prob*delta\n",
    "      critic_loss = delta**2\n",
    "\n",
    "      self.actor_optimizer.zero_grad()\n",
    "      self.critic.zero_grad()\n",
    "      \n",
    "      (actor_loss + critic_loss).backward()\n",
    "      # critic_loss.backward()\n",
    "      # for param in self.q_value_dqn.parameters():\n",
    "      #     param.grad.data.clamp_(-1, 1)\n",
    "      self.actor_optimizer.step()\n",
    "      self.critic_optimizer.step()\n",
    "\n",
    "      return actor_loss.item(), critic_loss.item()\n",
    "      \n",
    "    def learn(self):\n",
    "      \n",
    "      for episode in tqdm(range(0, self.max_episodes), unit='episodes'):\n",
    "        obs = self.env.reset()[0]\n",
    "        done = False\n",
    "        reward_sum = 0\n",
    "        steps = 0\n",
    "        a_running_loss=0\n",
    "        c_running_loss=0\n",
    "        I = 1\n",
    "        current_state = torch.from_numpy(obs).unsqueeze(0) if self.typ==\"dense\" else self.screen_preprocessor(self.env.render())\n",
    "        while not done:\n",
    "          action_prob = self.actor(current_state)\n",
    "          actions_cat = Categorical(action_prob)\n",
    "\n",
    "          current_action = actions_cat.sample()\n",
    "          action_log_prob = actions_cat.log_prob(current_action)\n",
    "\n",
    "          curr_obs, curr_reward, done, truncated, info = self.env.step(current_action.item())\n",
    "\n",
    "          reward_sum += curr_reward\n",
    "          \n",
    "          next_state = torch.from_numpy(curr_obs).unsqueeze(0) if self.typ==\"dense\" else self.screen_preprocessor(self.env.render())\n",
    "  \n",
    "          actor_loss, critic_loss = self.train(current_state, curr_reward, next_state, done, action_log_prob, I)\n",
    "          I = self.gamma*I\n",
    "          steps +=1\n",
    "          a_running_loss += actor_loss\n",
    "          c_running_loss += critic_loss\n",
    "          current_state = next_state\n",
    "\n",
    "          if steps==self.max_steps or reward_sum==470:\n",
    "            done=True\n",
    "\n",
    "        # self.update_target_net(episode)\n",
    "            \n",
    "        self.timestep_list.append(steps)\n",
    "        self.rewards_list.append(reward_sum)\n",
    "        self.epsilon_list.append(self.epsilon)\n",
    "        self.epsilon = max(0.01, self.epsilon*self.epsilon_decay_lamda)\n",
    "      \n",
    "        if episode % 100 == 99:\n",
    "          # print(f\"Buffer_size: {len(self.replay_buffer)}|Train_time:{sum(self.tm[-100:])/100}| Sub_t:{sum(self.tr[-100:])/100}\")\n",
    "          # print(f'[Episode: {episode+1}] - Steps: {steps+1} | Rewards: {reward_sum+1} | Epsilon: {self.epsilon} | loss: {running_loss / 10:.3f}')\n",
    "          print(f'[Episode: {episode+1}] - Steps: {sum(self.timestep_list[-100:])/100} | Rewards: {sum(self.rewards_list[-100:])/100} | Epsilon: {self.epsilon} | loss: {a_running_loss / 10:.3f}, {c_running_loss / 10:.3f}')\n",
    "          running_loss = 0.0\n",
    "          if sum(self.rewards_list[-100:])/100 >= 470:\n",
    "            break\n",
    "          # p.print()\n",
    "        \n",
    "    def update_target_net(self, episode):\n",
    "       if episode % self.target_update_frequency == self.target_update_frequency-1:   \n",
    "          self.critic.load_state_dict(self.actor.state_dict())\n",
    "    \n",
    "    def get_minibatch_from_replay_buffer(self):\n",
    "      exp_batch = random.sample(self.replay_buffer, self.buffer_minibatch_size) #max(len(self.replay_buffer), self.buffer_minibatch_size))\n",
    "      current_state_batch = torch.cat(tuple([exp[0] for exp in exp_batch]),dim=0)\n",
    "      current_action_batch = torch.cat(tuple([exp[1] for exp in exp_batch]),dim=0)\n",
    "      next_state_batch = torch.cat(tuple([exp[3] for exp in exp_batch]),dim=0)\n",
    "      current_reward_batch = torch.tensor(tuple([exp[2] for exp in exp_batch]))#.to(self.device)\n",
    "      current_done_batch = torch.tensor(tuple([exp[4] for exp in exp_batch]))#.to(self.device)\n",
    "      return current_state_batch, current_action_batch, next_state_batch, current_reward_batch, current_done_batch\n",
    "    \n",
    "    def add_to_replay_buffer(self, experience):\n",
    "       self.replay_buffer.append(experience)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_27431/3958534277.py:17: DeprecationWarning: CUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "  T.Resize(84, interpolation=Image.CUBIC),\n",
      "  1%|          | 105/10000 [00:04<05:22, 30.68episodes/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Episode: 100] - Steps: 23.0 | Rewards: 23.0 | Epsilon: 0.9332543007969923 | loss: 2.001, 3.007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 206/10000 [00:08<05:19, 30.66episodes/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Episode: 200] - Steps: 22.93 | Rewards: 22.93 | Epsilon: 0.8709635899560828 | loss: 0.842, 1.282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 304/10000 [00:12<06:08, 26.30episodes/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Episode: 300] - Steps: 22.11 | Rewards: 22.11 | Epsilon: 0.8128305161641024 | loss: 1.641, 2.464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 401/10000 [00:15<05:19, 30.07episodes/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Episode: 400] - Steps: 22.44 | Rewards: 22.44 | Epsilon: 0.758577575029188 | loss: 0.927, 1.456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 491/10000 [00:19<06:16, 25.25episodes/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [56], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m env\u001b[39m.\u001b[39mreset()\n\u001b[1;32m      3\u001b[0m agent \u001b[39m=\u001b[39m CartPoleVanillaDQN(env)\n\u001b[0;32m----> 4\u001b[0m agent\u001b[39m.\u001b[39;49mlearn()\n",
      "Cell \u001b[0;32mIn [55], line 149\u001b[0m, in \u001b[0;36mCartPoleVanillaDQN.learn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m done:\n\u001b[1;32m    148\u001b[0m   action_prob \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactor(current_state)\n\u001b[0;32m--> 149\u001b[0m   actions_cat \u001b[39m=\u001b[39m Categorical(action_prob)\n\u001b[1;32m    151\u001b[0m   current_action \u001b[39m=\u001b[39m actions_cat\u001b[39m.\u001b[39msample()\n\u001b[1;32m    152\u001b[0m   action_log_prob \u001b[39m=\u001b[39m actions_cat\u001b[39m.\u001b[39mlog_prob(current_action)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/distributions/categorical.py:66\u001b[0m, in \u001b[0;36mCategorical.__init__\u001b[0;34m(self, probs, logits, validate_args)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_events \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_param\u001b[39m.\u001b[39msize()[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m     65\u001b[0m batch_shape \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_param\u001b[39m.\u001b[39msize()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_param\u001b[39m.\u001b[39mndimension() \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m torch\u001b[39m.\u001b[39mSize()\n\u001b[0;32m---> 66\u001b[0m \u001b[39msuper\u001b[39;49m(Categorical, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(batch_shape, validate_args\u001b[39m=\u001b[39;49mvalidate_args)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/distributions/distribution.py:49\u001b[0m, in \u001b[0;36mDistribution.__init__\u001b[0;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[1;32m     45\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m does not define `arg_constraints`. \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m\n\u001b[1;32m     46\u001b[0m                   \u001b[39m'\u001b[39m\u001b[39mPlease set `arg_constraints = \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m` or initialize the distribution \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m\n\u001b[1;32m     47\u001b[0m                   \u001b[39m'\u001b[39m\u001b[39mwith `validate_args=False` to turn off validation.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     48\u001b[0m \u001b[39mfor\u001b[39;00m param, constraint \u001b[39min\u001b[39;00m arg_constraints\u001b[39m.\u001b[39mitems():\n\u001b[0;32m---> 49\u001b[0m     \u001b[39mif\u001b[39;00m constraints\u001b[39m.\u001b[39;49mis_dependent(constraint):\n\u001b[1;32m     50\u001b[0m         \u001b[39mcontinue\u001b[39;00m  \u001b[39m# skip constraints that cannot be checked\u001b[39;00m\n\u001b[1;32m     51\u001b[0m     \u001b[39mif\u001b[39;00m param \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mgetattr\u001b[39m(\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m), param), lazy_property):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1', render_mode=\"rgb_array\")\n",
    "env.reset()\n",
    "agent = CartPoleVanillaDQN(env)\n",
    "agent.learn()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
